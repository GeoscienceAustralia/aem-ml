data:
    aem_folder: 'aem_data'
    train_data:
        aem_train_data:
            - 'train.shp'
        targets:
            - 'interpretation_zone53_albers_study_area_Ceno_depth.shp'
        weights:
            - 1
    apply_model: 'oos.shp'  # this is for prediction on uninterpreted data
    weight_col: 'BoundConf'
    line_col: 'SURVEY_LIN'
    conductivity_columns_prefix: 'cond'
    thickness_columns_prefix: 'thick'
    aem_covariate_cols:
        - 'ceno_euc_a'
        - 'Gravity_la'
        - 'national_W'
        - 'relief_ele'
        - 'relief_mrv'
        - 'SagaWET9ce'
        - 'elevation'
        - 'tx_height'
    # aem line scan radius in meters - aem points are assumed to be on the same flight line under this radius
    aem_line_scan_radius: 500
    # aem lines are split into batches of aem_line_splits's to generate artificial splits
    aem_line_splits: 1000
    test_train_split:
        train: 0.6
        val: 0.2
        test: 0.2
    rows: -1
    oos_validation:
        aem_validation_data:
            - 'oos.shp'
        targets:
            - 'interpretation_zone53_albers_study_area_Ceno_depth.shp'

# will run n_iter * cv number of jobs
# n_iter: Number of parameter settings that are sampled.
#n_jobs: int, default=1
#    Number of jobs to run in parallel. At maximum there are
#    ``n_points`` times ``cv`` jobs available during each iteration.
#
#n_points: int, default=1
#    Number of parameter settings to sample in parallel. If this does
#    not align with ``n_iter``, the last iteration will sample less
#    points.

learning:
    algorithm: xgboost
    params:
        objective: reg:squarederror
        max_depth: 8
        learning_rate: 0.17419472963030241
        n_estimators: 24
        colsample_bynode: 0.5469102757224991
        colsample_bylevel: 0.9793166601242764
        colsample_bytree: 0.11576985253405572
        max_delta_step: 15
        gamma: 0.7739716978240518
        reg_alpha: 2.857852261835614
        reg_lambda: 7.925780530694022
        subsample: 0.11757957586875012
        n_jobs: -1
        random_state: 3
        booster: gbtree
        min_child_weight: 3
    cross_validation:
        kfold: 3
    weighted_model:
        weights:
            H: 2
            M: 1
            L: 0.5
    numpy_seed: 10
    include_aem_covariates: true
    include_thickness: true
    include_conductivity_derivatives: true
    smooth_twod_covariates: true
    smooth_covariates_kernel_size: (21, 9)
    # depth - columns (x) -- 30 sections
    # along the line is - rows (y) -- many 100s of kms with resolutionof 12meters
    optimisation:
#        searchcv_params:
#            n_iter: 6
#            cv: 2
#            verbose: 1000
#            n_points: 3
#            n_jobs: 6
#        params_space:
#            'max_depth': Integer(1, 15)
#            'learning_rate': Real(10 ** -5, 10 ** 0, prior="log-uniform")
#            'n_estimators': Integer(2, 20)
#            'min_child_weight': Integer(1, 10)
#            'max_delta_step': Integer(0, 10)
#            'gamma': Real(0, 0.5, prior="uniform")
#            'colsample_bytree': Real(0.3, 0.9, prior="uniform")
#            'subsample': Real(0.01, 1.0, prior='uniform')
#            'colsample_bylevel': Real(0.01, 1.0, prior='uniform')
#            'colsample_bynode': Real(0.01, 1.0, prior='uniform')
#            'reg_alpha': Real(1, 100, prior='uniform')
#            'reg_lambda': Real(0.01, 10, prior='log-uniform')
        hyperopt_params:
            max_evals: 5
            step: 2
            cv: 3
            verbose: true
            random_state: 3
            scoring: r2  # r2, neg_mean_absolute_error, etc..see note above
            algo: bayes   # bayes, or anneal
        hp_params_space:
            max_depth: randint('max_depth', 1, 15)
            n_estimators: randint('n_estimators', 5, 25)
            learning_rate: loguniform('learning_rate', -5, 0)
            booster: choice('booster', ['gbtree', 'dart'])
            min_child_weight: randint('min_child_weight', 0, 10)
            max_delta_step: randint('max_delta_step', 0, 20)
            gamma: uniform('gamma', 0, 10)
            subsample: uniform('subsample', 0.01, 1.0)
            colsample_bytree: uniform('colsample_bytree', 0.01, 1.0)
            colsample_bylevel: uniform('colsample_bylevel', 0.01, 1.0)
            colsample_bynode: uniform('colsample_bynode', 0.01, 1.0)
            reg_alpha: uniform('reg_alpha', 0, 100)
            reg_lambda: loguniform('reg_lambda', 0.01, 10)


output:
    directory: out/xgboost/
    train:
        covariates_csv: true
        true_vs_pred: true
        feature_ranking: true
    pred:
        quantiles: 0.95
        optimised_model: true
        covariates_csv: true
        pred: true
